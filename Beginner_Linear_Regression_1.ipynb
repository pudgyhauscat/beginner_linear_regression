{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOO1Ka/bo1yt5kIzOXv/O5W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pudgyhauscat/beginner_linear_regression/blob/main/Beginner_Linear_Regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IOgANF6apguo"
      },
      "outputs": [],
      "source": [
        "#below is code for a linear regression on a generated data set with an equation \n",
        "#of y = 2 + 3*x_1 + x_2 - 6*x_3 and y = 2 + 3*x_1 + x_2 - 6*x_3 + u. \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "#initialize three independent variables. each variable has 50 observations with \n",
        "#their lowest value possible at 1 and highest possible value at 50. all of this \n",
        "#info can be customized. \n",
        "\n",
        "x_1 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "x_2 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "x_3 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "\n",
        "#calculate the dependent variable based on the independent variable and the \n",
        "#intercept. we're using the variable i to stand for an index so that the list \n",
        "#comprehension can effectively move through two lists of the same size at the \n",
        "#same time.\n",
        "\n",
        "linear_y = [3*x for x in x_1]\n",
        "linear_y_2 = [linear_y[i] + x_2[i] for i in range(0,len(x_2))]\n",
        "linear_y_3 = [linear_y_2[i] - 6*x_3[i] for i in range(0, len(x_3))]\n",
        "linear_y_3 = np.array(linear_y_3) + 2\n",
        "\n",
        "#we have the option to add noise to the model. here we opt to add normally \n",
        "#distributed noise, which is a standard assumption of the model. later we'll use \n",
        "#the dependent variable with noise and a dependent variable without noise. to \n",
        "#compare outcomes\n",
        "\n",
        "noise = np.random.normal(10, 3, 100)\n",
        "linear_y_3_with_noise = [linear_y_3[i] + noise[i] for i in range(0, len(noise))]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here we'll create a matrix using our previously declared independent variables \n",
        "#called X_0. Linear regression is an equation of the form Ax = b, so this step \n",
        "#creates A. Designated capital X_0. we have b we want x. training the model is \n",
        "#simple once the data is set. Worth noting is that sk_learn's regression model \n",
        "#is going to use a single value decomposition, which is computationally \n",
        "#different than other methods we might use. \n",
        "\n",
        "X_0 = np.column_stack([x_1, x_2, x_3])\n",
        "sk_learn_regression_noiseless = LinearRegression()\n",
        "sk_learn_regression_noiseless.fit(X_0, linear_y_3)\n",
        "sk_learn_regression_noiseless.intercept_, sk_learn_regression_noiseless.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrc5XM4A4kOX",
        "outputId": "3c9406d9-ca5f-4425-914f-16ad4d45c897"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.000000000000014, array([ 3.,  1., -6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here the same X_0 from above is used. we substitute linear_y_3 with \n",
        "#linear_y_3_with_noise. it changes the outcomes of the regression. we will \n",
        "#struggle to get the actual parameters because of the error we've introduced. \n",
        "\n",
        "sk_learn_regression_with_noise = LinearRegression()\n",
        "sk_learn_regression_with_noise.fit(X_0, linear_y_3_with_noise)\n",
        "sk_learn_regression_with_noise.intercept_, sk_learn_regression_with_noise.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v6-4xE7ANr3",
        "outputId": "2d91102b-941b-4b07-841e-70b6fd2fe8ad"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14.382809971178743, array([ 2.96997359,  0.93850205, -6.10174819]))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generally we'd split the data into a train and test set. here we split the observations\n",
        "#and the noiseless data. we train the model with a simple call to LinearRegression() from\n",
        "#sklearn. then we just use the .split method and specific our dependent and independent variables\n",
        "\n",
        "x_test_noiseless, x_train_noiseless, y_test_noiseless, y_train_noiseless = train_test_split(X_0, linear_y_3, test_size = .2)\n",
        "\n",
        "sk_learn_regression_with_split_noiseless= LinearRegression()\n",
        "sk_learn_regression_with_split_noiseless.fit(x_train_noiseless, y_train_noiseless)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcQXayyrAEeP",
        "outputId": "ec354bc9-331e-4068-ce23-0ebaa4cdacb9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we see that in the noisless model the coefficients are found on the training set\n",
        "\n",
        "sk_learn_regression_with_split_noiseless.intercept_, sk_learn_regression_with_split_noiseless.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6CGC7iNhG-Z",
        "outputId": "2fad5ca7-1baf-4935-923e-af5c9eeabdb4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.000000000000007, array([ 3.,  1., -6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use the model built on the trian set to make predictions on the test set. \n",
        "#we use the standard mean squared error for linear regression and get a very small error\n",
        "\n",
        "predictions_with_split_noiseless = sk_learn_regression_with_split_noiseless.predict(x_test_noiseless)\n",
        "\n",
        "metrics.mean_squared_error(predictions_with_split_noiseless, y_test_noiseless)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbcpkZtC-jT_",
        "outputId": "8cc364f0-9279-4c19-f5b3-39a0800a993a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7544064581923155e-28"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here is a split for a regression with noise and the model training process \n",
        "\n",
        "x_test_noise, x_train_noise, y_test_noise, y_train_noise = train_test_split(X_0, linear_y_3_with_noise, test_size = .2)\n",
        "\n",
        "sk_learn_regression_with_split_noise= LinearRegression()\n",
        "sk_learn_regression_with_split_noise.fit(x_train_noise, y_train_noise)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noDhATW-EYzW",
        "outputId": "766069ed-5c7c-4a9b-8f92-fa072f61b250"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we see that in the noisless model the coefficients are not found exactly on the training\n",
        "#set with noise\n",
        "\n",
        "sk_learn_regression_with_split_noise.intercept_, sk_learn_regression_with_split_noise.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-WoaoWVK8LB",
        "outputId": "ddf4111c-e288-4c12-c297-9cce0cf3637b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12.436620787837544, array([ 3.1072658 ,  0.96871125, -6.0698519 ]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#since we didn't find the exact parameters we're left with less accurate predictions.\n",
        "#this is much more likely than the above outcome. train_test_split won't always split \n",
        "#the data in the same way based on how we configured it above. so, the model will have varying\n",
        "#coefficients and varying errors. in practice we'd want to find the best model that doesn't\n",
        "#overfit or underfit\n",
        "\n",
        "predictions_with_split_noise = sk_learn_regression_with_split_noise.predict(x_test_noise)\n",
        "\n",
        "metrics.mean_squared_error(predictions_with_split_noise, y_test_noise)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTRyyfhZQjDH",
        "outputId": "f701c250-1755-4ba1-e618-a302ac4b1ba2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.527457668676409"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the data generation step included above can be fit into a class and ported out of this program\n",
        "#this will be useful, because we're going to use the same data generation steps in later notebooks. \n",
        "#this class will initialize data sets in the exact same way that we initialized the data set here and \n",
        "#we won't waste time coding it later\n",
        "\n",
        "class data_set_generator:\n",
        "    x_1 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "    x_2 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "    x_3 = np.random.randint(low = 1, high = 20, size = 100)\n",
        "    \n",
        "    def __init__(self):\n",
        "      self.intercept = np.random.randint(low = 1, high = 20, size = 1)\n",
        "      self.X_0 = np.column_stack([x_1, x_2, x_3])\n",
        "      \n",
        "    def calculate_y(self, weight_1, weight_2, weight_3):\n",
        "      linear_y = [weight_1*x for x in x_1]\n",
        "      linear_y_2 = [linear_y[i] + weight_2*x_2[i] for i in range(0,len(x_2))]\n",
        "      linear_y_3 = [linear_y_2[i] - weight_3*x_3[i] for i in range(0, len(x_3))]\n",
        "      self.linear_y = np.array(linear_y_3) + self.intercept\n",
        "      noise = noise = np.random.normal(10, 3, 100)\n",
        "      self.linear_y_noise = [linear_y_3[i] + noise[i] for i in range(0, len(noise))]\n"
      ],
      "metadata": {
        "id": "QwWpnVVG1dhm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we'll save the class to a variable and supply some weights\n",
        "\n",
        "data = data_set_generator()\n",
        "data.calculate_y(3, 1, 6)"
      ],
      "metadata": {
        "id": "y4oCpCfPgBYB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we'll check that the class is behaving as we expect by doing a regression with it\n",
        "#the only difference is that we have the class randomly generating the intercept.\n",
        "\n",
        "sk_learn_regression_class_test = LinearRegression()\n",
        "sk_learn_regression_class_test.fit(data.X_0, data.linear_y)\n",
        "sk_learn_regression_class_test.intercept_, sk_learn_regression_class_test.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmsLccf2jATh",
        "outputId": "8f898c6f-1509-46a7-af53-17fa489ad04b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0000000000000142, array([ 3.,  1., -6.]))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}